{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1\n",
    "#Import libraries\n",
    "from io import StringIO\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import fitz\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from dframcy import DframCy\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2\n",
    "#Set up directory paths and column names\n",
    "pdf_dir = \"/your/pdf/directory/\"\n",
    "csv_dir = \"/your/csv/directory/\"\n",
    "csv_name = \"repertoire.csv\"\n",
    "\n",
    "columns = ['TITLE','DATE','TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3\n",
    "#Migrate TXT to CSV\n",
    "\n",
    "#Itirate through the pdf directory\n",
    "corpus = (f for f in os.listdir(pdf_dir) if not f.startswith('.') and isfile(join(pdf_dir, f)))\n",
    "#Create a dataframe where we save text from each pdf\n",
    "df = pd.DataFrame(columns=columns)\n",
    "#Iterate through each pdf\n",
    "for filename in corpus:\n",
    "    output_string = StringIO()\n",
    "    #scrape OCR layer from pdf\n",
    "    text_file = fitz.open(join(pdf_dir,filename))\n",
    "    #title is the filename minus the .pdf ending\n",
    "    title = filename[:-4]\n",
    "    #In my corpus, the date is embeded in the filename. \n",
    "    #This series of code generates a date field.\n",
    "    start = len(filename) - 14\n",
    "    date = filename[start:-4]\n",
    "    date = date.replace('_','-')\n",
    "    #Now iterate through each page of the pdf\n",
    "    for page in text_file:\n",
    "        output_string.write(page.getText(\"text\"))\n",
    "    #the text generated from the above for loop is saved in the doc object\n",
    "    doc = output_string.getvalue()\n",
    "    #Here I search the doc object for the first word or phrase that identifies the portion of text that\n",
    "    #I want to extract. This process took refining to find the best regular expression\n",
    "    first = re.search(r\"place|regular|expressions|here\",doc)\n",
    "    if line = \"None\"\n",
    "        \n",
    "    else:\n",
    "        length = len(doc)\n",
    "        #I shorten the doc object to include text that appears on and after the first word of the text.\n",
    "        doc_end = doc[first.start():length]\n",
    "        #Here I search the doc object for the last word or phrase that identifies the end of the text that\n",
    "        #I want to extract. This process took refining to find the best regular expression\n",
    "        last = re.search(r\"place|regular|expressions|here\",doc_end)\n",
    "    if last == None:\n",
    "        line = doc_end\n",
    "    else:\n",
    "        line = doc_end[:last.start()]\n",
    "    \n",
    "    line = line.replace('/n',' ')\n",
    "    #I create a list with the title, date and text from each file\n",
    "    data = [title, date, line]\n",
    "    #I turn the list into a pandas series with the index of my dataframe's column names\n",
    "    data_s = pd.Series(data, index=df.columns)\n",
    "    #I finally append the panda series to my dataframe as a row.\n",
    "    df = df.append(data_s,ignore_index=True)\n",
    "#I sort dataframe by date.\n",
    "df = df.sort_values(by=['DATE'])\n",
    "#I save dataframe as a csv file.\n",
    "df.to_csv(join(csv_dir, csv_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4\n",
    "#Save the dates that came back with a nill value\n",
    "#With this csv file, I can check that I didn't miss an advertisement\n",
    "csv_name = 'repertoire.csv'\n",
    "df = pd.read_csv(join(csv_dir,csv_name))\n",
    "missing = df[df['TEXT'] == \"None\"]\n",
    "csv_name = \"missing_repertoire.csv\"\n",
    "path = \"/your/path/here/\"\n",
    "\n",
    "missing.to_csv(join(path,csv_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5\n",
    "#Load NLP language model with spaCy\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6\n",
    "#Run the random date function to prepare for the creation of a training set\n",
    "def random_dates(series1):\n",
    "    import random\n",
    "\n",
    "    dates = [d for d in series1]\n",
    "    dates.sort()  # make sure that the filenames have a fixed order before shuffling\n",
    "    random.seed(230)\n",
    "    random.shuffle(dates) # shuffles the ordering of filenames (deterministic given the chosen seed)\n",
    "\n",
    "    split_1 = int(0.05 * len(dates))\n",
    "    train_dates = dates[:split_1]\n",
    "    \n",
    "    return train_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7\n",
    "#Reload the csv file with raw data\n",
    "csv_dir = \"/your/csv/directory/\"\n",
    "csv_name = \"repertoire.csv\"\n",
    "\n",
    "columns = ['TITLE','DATE','TEXT']\n",
    "df = pd.read_csv(join(csv_dir,csv_name))\n",
    "\n",
    "# This step generates a series of randomly selected dates that creates tagged csv files. \n",
    "columns = ['Word','Tag','Date']\n",
    "#create an empty pandas dataframe to store NLP data with text\n",
    "tagged_df = pd.DataFrame(columns = columns)\n",
    "#Put in the path where you want traininig set csv files stored\n",
    "training_path = '/your/training/path/here'\n",
    "\n",
    "#Filter out any dates that doesn't have an advertisement. \n",
    "all_dates_df = df[df['TEXT'] != \"None\"]\n",
    "#generate a random selection of dates\n",
    "dates = random_dates(all_dates_df['DATE'].unique())\n",
    "#iterate through dates\n",
    "for day in dates:\n",
    "    tagged_df = pd.DataFrame(columns = columns)\n",
    "    #Pass the text value from \n",
    "    text = df['TEXT'][df['DATE'] == day].values\n",
    "    #create a dataframe from the nlp model\n",
    "    dframcy = DframCy(nlp)\n",
    "    #transform text into a tagged NLP object with parts of speech and entity tages\n",
    "    doc = dframcy.nlp(text)\n",
    "    #transform doc into a dataframe with parts of speech and entity tags\n",
    "    annotation_dataframe = dframcy.to_dataframe(doc)\n",
    "    #set up name of csv training file\n",
    "    f_name = \"training_csv_{}.csv\".format(day)\n",
    "    #save dataframe into a csv traininig file\n",
    "    annotation_dataframe.to_csv(join(training_path,f_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 8 - Manually tag the training CSV Files. Add entity tags under the \"token_tag_\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 9 - Run the training set python script \"semi_automate_model.py\"- this script cannot run in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 10 - Once you have run semi_automate_model.py. Reload spaCy's nlp model with trained data\n",
    "nlp = spacy.load('/path/to/your/model/location/here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 11 - Load NER function\n",
    "def return_NER(sentence):\n",
    "    # Tokenize a phrase\n",
    "    doc = nlp(sentence)\n",
    "    # Return the text and label for each entity\n",
    "    return [(X.text, X.label_) for X in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 12 - Generate CSV with Tagged Entities from Newspaper Advertisements \n",
    "csv_dir = \"/path/to/your/csv/files\"\n",
    "#here is the csv filename with your raw data\n",
    "raw_csv = \"raw_data.csv\"\n",
    "#here is the csv filename for your tagged dataframe\n",
    "final_csv = \"final_data.csv\"\n",
    "#reload the dataframe with your raw data\n",
    "df = pd.read_csv(join(csv_dir,raw_csv))\n",
    "#set up a dataframe for your tagged data\n",
    "columns = ['Title','Word','Tag','Date']\n",
    "tagged_df = pd.DataFrame(columns = columns)\n",
    "\n",
    "dates = df['DATE']\n",
    "#iterate through the dates of your dataframe\n",
    "for day in dates:\n",
    "    #retrieve text for the date\n",
    "    text = df['TEXT'][df['DATE'] == day].values\n",
    "    #retrive the title for the date\n",
    "    title = df['TITLE'][df['DATE'] == day].values\n",
    "    #Tokenize the text - This generates a series of words and tags\n",
    "    data = return_NER(text[0])\n",
    "    #Add tokenized text to a new dataframe\n",
    "    new_df = pd.DataFrame(data, columns =['Word', 'Tag'])\n",
    "    #add title to a new dataframe\n",
    "    new_df['Title'] = title[0]\n",
    "    #add date to a new dataframe\n",
    "    new_df['Date'] = day\n",
    "    #append data to tagged dataframe\n",
    "    tagged_df = tagged_df.append(new_df, ignore_index=True)\n",
    "#save tagged dataframe to final csv file\n",
    "tagged_df.to_csv(join(csv_dir, final_csv))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
